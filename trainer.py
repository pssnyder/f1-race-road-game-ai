"""
ğŸ‹ï¸ F1 Race AI Training System ğŸ¤–
=================================

Welcome to the AI Training Center! ğŸ“âœ¨    # Calculate the decay rate needed to reach epsilon_end at the target episode
    # Using: epsilon_end = epsilon_start * (decay_rate ^ target_episodes)
    # Solving: decay_rate = (epsilon_end / epsilon_start) ^ (1 / target_episodes)
    import math
    
    print(f"ğŸ¯ ADAPTIVE EXPLORATION SYSTEM")
    print(f"   ğŸ“Š Total episodes: {episodes}")
    print(f"   ğŸ“‰ Exploration ends at episode: {exploration_end_episode} ({exploration_completion_ratio*100:.0f}% through training)")
    print(f"   ğŸ“ˆ Exploration range: {EXPLORATION_START:.2f} â†’ {EXPLORATION_END:.3f}")
    print(f"   ğŸ”„ Decay type: {exploration_decay_type}")
    
    if exploration_end_episode > 0:
        if exploration_decay_type.lower() == "linear":
            # Linear decay: epsilon decreases by fixed amount each episode
            EXPLORATION_DECAY = "linear"  # Special flag for linear decay
            linear_decay_per_episode = (EXPLORATION_START - EXPLORATION_END) / exploration_end_episode
            print(f"   ğŸ“‰ Linear decay per episode: -{linear_decay_per_episode:.6f}")
        else:
            # Exponential decay: epsilon multiplied by decay factor each episode
            EXPLORATION_DECAY = math.pow(EXPLORATION_END / EXPLORATION_START, 1.0 / exploration_end_episode)
            print(f"   ğŸ“‰ Exponential decay rate: {EXPLORATION_DECAY:.6f}")
    else:
        EXPLORATION_DECAY = 0.9995  # Fallback to previous default
        print(f"   âš ï¸  Using fallback decay rate: {EXPLORATION_DECAY:.6f}")ere the magic happens - where we transform a clueless AI into a racing expert!
Think of this as the "gym" where our artificial race car driver learns to become a champion.

ğŸ¯ WHAT HAPPENS DURING TRAINING?
-------------------------------
Imagine teaching a friend to drive:
- ğŸš— They start by crashing into everything (random actions)  
- ğŸ’¡ Gradually they learn "don't hit red things" (negative rewards)
- ğŸ† Eventually they become skilled drivers (positive rewards)
- ğŸ“ˆ Each practice session makes them better (learning from experience)

Our AI goes through the same process, but MUCH faster! ğŸš€

ğŸ§  THE TRAINING PROCESS:
1. ğŸ® CREATE ENVIRONMENT: Set up the racing game
2. ğŸ¤– CREATE AI AGENT: Initialize the "student driver" 
3. ğŸ”„ PRACTICE LOOP: Let AI play thousands of games
4. ğŸ“Š TRACK PROGRESS: Monitor how well AI is learning
5. ğŸ’¾ SAVE RESULTS: Keep the trained "brain" for later use

ğŸ“ TRAINING PHASES:
- ğŸŒŸ EXPLORATION: AI tries random actions to discover what works
- ğŸ§  LEARNING: AI updates its "brain" based on rewards/penalties  
- ğŸ¯ EXPLOITATION: AI uses learned knowledge to get better scores
- ğŸ† MASTERY: AI becomes expert driver that rarely crashes!

ğŸ‘¥ AUDIENCE NOTES:
- ğŸ”¬ Data Scientists: Notice the hyperparameter tuning and performance metrics
- ğŸ‘©â€ğŸ« Educators: Great example of trial-and-error learning scaled up
- ğŸ‘¶ Young Coders: Watch a computer learn to play games just like humans!
- ğŸ¤“ AI Curious: See reinforcement learning in action with real-time feedback

Author: Pat Snyder ğŸ’»
Created for: Learning Labs Portfolio ğŸŒŸ
"""

import numpy as np
import matplotlib.pyplot as plt
from environment import F1RaceEnvironment
from agent import DQNAgent
import time
import os
import signal
import sys

import signal
import sys

# Global flag for graceful shutdown
graceful_shutdown = False

def signal_handler(signum, frame):
    """Handle Ctrl+C gracefully"""
    global graceful_shutdown
    print("\n\nğŸ›‘ GRACEFUL SHUTDOWN REQUESTED")
    print("=" * 50)
    print("ğŸ”„ Finishing current episode and saving progress...")
    print("ğŸ’¾ Please wait for clean shutdown...")
    print("   (Press Ctrl+C again to force quit)")
    graceful_shutdown = True
    
    # Set up handler for second Ctrl+C to force quit
    signal.signal(signal.SIGINT, lambda s, f: sys.exit(1))

# Set up the signal handler
signal.signal(signal.SIGINT, signal_handler)

def train_racing_ai(episodes=2000, target_update_frequency=100, save_frequency=500, 
                   show_training=False, resume_checkpoint: str | None = None,
                   framerate_multiplier: int = 100, chart_update_frequency: int = 100,
                   exploration_completion_ratio: float = 0.8, exploration_decay_type: str = "exponential"):
    """
    ğŸ‹ï¸ Train the AI to become an expert F1 race car driver!
    
    This function runs the complete training process where the AI plays
    thousands of racing games and learns from each experience.
    
    Args:
        episodes (int): How many games to play (2000 = good for learning)
        target_update_frequency (int): How often to update target network (100 = stable)
        save_frequency (int): How often to save progress (500 = reasonable backup)
        show_training (bool): Whether to show the game while training (False = faster)
        resume_checkpoint (str | None): Path to checkpoint to resume from
        framerate_multiplier (int): Speed multiplier 1-500 (100 = normal speed, 200 = 2x speed)
        chart_update_frequency (int): How often to update training charts (100 = every 100 episodes)
        exploration_completion_ratio (float): At what fraction of training should exploration reach minimum (0.8 = 80%)
        exploration_decay_type (str): Type of decay curve - "exponential" or "linear" (exponential = smooth curve, linear = straight line)
        
    Returns:
        DQNAgent: The trained AI agent
    """
    
    print("ğŸ® INITIALIZING F1 RACE AI TRAINING SYSTEM")
    print("=" * 50)
    
    # ğŸ›ï¸ TRAINING CONFIGURATION - Easy to modify!
    # ===========================================
    LEARNING_RATE = 0.001        # ğŸ“š How fast AI learns (0.001 = stable default)
    DISCOUNT_FACTOR = 0.95       # ğŸ”® How much AI cares about future (0.95 = forward-thinking)
    EXPLORATION_START = 1.0       # ğŸ² Initial randomness (100% random at start)
    EXPLORATION_END = 0.01       # ğŸ² Final randomness (1% random when expert)  
    MEMORY_SIZE = 15000         # ğŸ§  How many experiences to remember
    BATCH_SIZE = 64             # ğŸ“¦ How many experiences to learn from at once
    
    # ğŸ“ DYNAMIC EXPLORATION DECAY CALCULATION
    # ========================================
    # Calculate when exploration should reach minimum (based on total episodes)
    exploration_end_episode = int(episodes * exploration_completion_ratio)
    
    # Calculate the decay rate needed to reach epsilon_end at the target episode
    # Using: epsilon_end = epsilon_start * (decay_rate ^ target_episodes)
    # Solving: decay_rate = (epsilon_end / epsilon_start) ^ (1 / target_episodes)
    import math
    if exploration_end_episode > 0:
        EXPLORATION_DECAY = math.pow(EXPLORATION_END / EXPLORATION_START, 1.0 / exploration_end_episode)
    else:
        EXPLORATION_DECAY = 0.9995  # Fallback to previous default
    
    print(f"ğŸ¯ ADAPTIVE EXPLORATION SYSTEM")
    print(f"   ï¿½ Total episodes: {episodes}")
    print(f"   ğŸ“‰ Exploration ends at episode: {exploration_end_episode} ({exploration_completion_ratio*100:.0f}% through training)")
    print(f"   ğŸ² Calculated decay rate: {EXPLORATION_DECAY:.6f}")
    print(f"   ğŸ“ˆ Exploration range: {EXPLORATION_START:.2f} â†’ {EXPLORATION_END:.3f}")
    print()
    
    # ğŸ—ï¸ CREATE TRAINING ENVIRONMENT AND AI AGENT
    # ============================================
    print("ğŸï¸  Creating racing environment...")
    # When not showing training, max out framerate for fastest training
    effective_framerate = 500 if not show_training else framerate_multiplier
    env = F1RaceEnvironment(render=show_training, framerate_multiplier=effective_framerate)
    
    print("ğŸ¤– Creating AI agent...")
    agent = DQNAgent(
        state_size=env.state_space_size,      # 5 numbers describing game state
        action_size=env.action_space_size,    # 3 possible actions  
        learning_rate=LEARNING_RATE,
        gamma=DISCOUNT_FACTOR,
        epsilon_start=EXPLORATION_START,
        epsilon_end=EXPLORATION_END,
        epsilon_decay=EXPLORATION_DECAY,
        memory_size=MEMORY_SIZE,
        batch_size=BATCH_SIZE
    )
    
    # Set total episodes for adaptive exploration tracking
    agent.total_episodes = episodes
    agent.exploration_end_episode = exploration_end_episode
    agent.exploration_decay_type = exploration_decay_type
    if exploration_decay_type.lower() == "linear" and exploration_end_episode > 0:
        agent.linear_decay_per_episode = (EXPLORATION_START - EXPLORATION_END) / exploration_end_episode
    if resume_checkpoint and os.path.isfile(resume_checkpoint):
        print(f"ğŸ“‚ Resuming from checkpoint: {resume_checkpoint}")
        agent.load_agent(resume_checkpoint)
    
    # ğŸ“Š DISPLAY TRAINING CONFIGURATION
    # =================================
    print(f"ğŸ“ State space: {env.state_space_size} values")
    print(f"ğŸ¯ Action space: {env.action_space_size} actions")
    print(f"ğŸ“š Learning rate: {LEARNING_RATE}")  
    print(f"ğŸ”® Discount factor: {DISCOUNT_FACTOR}")
    print(f"ğŸ® Training episodes: {episodes}")
    print(f"ğŸ‘ï¸  Show training: {show_training}")
    print(f"âš¡ Framerate multiplier: {effective_framerate}% ({'MAX' if effective_framerate == 500 else 'normal' if effective_framerate == 100 else 'custom'})")
    print(f"ğŸ“Š Chart update frequency: every {chart_update_frequency} episodes")
    print("-" * 50)
    
    # ğŸ“Š TRAINING METRICS - Track AI's progress
    # ========================================
    # Initialize scores from checkpoint if resuming
    if resume_checkpoint and agent.episode_scores:
        all_scores = agent.episode_scores.copy()  # Include previous scores for averages
        all_episode_lengths = []  # Episode lengths aren't saved, so start fresh
        best_score_ever = max(agent.episode_scores)
        print(f"ğŸ“Š Restored {len(all_scores)} previous episodes with best score: {best_score_ever}")
    else:
        all_scores = []           # Score from each episode
        all_episode_lengths = []  # How long each episode lasted
        best_score_ever = 0       # Best score achieved so far
    
    training_start_time = time.time()
    
    # ğŸ‹ï¸ MAIN TRAINING LOOP
    # =====================
    print("ğŸš€ STARTING TRAINING!")
    print("Watch the AI transform from terrible to talented! ğŸ“")
    print("ğŸ’¡ Press Ctrl+C anytime for graceful shutdown (saves progress)")
    print()
    
    # If resuming, start from the current episode count
    start_episode = len(agent.episode_scores) if resume_checkpoint else 0
    if start_episode > 0:
        print(f"ğŸ“‚ Resuming from episode {start_episode} (continuing where we left off)")
    
    for episode_idx in range(episodes):
        # ğŸ›‘ CHECK FOR GRACEFUL SHUTDOWN
        # ==============================
        if graceful_shutdown:
            print(f"\nğŸ›‘ Graceful shutdown at episode {start_episode + episode_idx}")
            break
            
        episode_number = start_episode + episode_idx  # Actual episode number for display
        
        # ğŸ”„ START NEW EPISODE
        # ===================
        current_state = env.reset()
        total_reward = 0
        steps_taken = 0
        episode_start_time = time.time()
        
        # ğŸ® PLAY ONE COMPLETE GAME
        # ========================
        while True:
            # ğŸ›‘ CHECK FOR GRACEFUL SHUTDOWN DURING EPISODE
            # =============================================
            if graceful_shutdown:
                print(f"   ğŸ›‘ Graceful shutdown during episode {episode_number}")
                break
                
            # ğŸ¤” AI DECIDES WHAT TO DO
            # ========================
            action = agent.choose_action(current_state, training_mode=True)
            
            # ğŸ¬ TAKE ACTION IN GAME  
            # ======================
            next_state, reward, game_over, info = env.step(action)
            
            # ğŸ’¾ STORE EXPERIENCE FOR LEARNING
            # ================================
            agent.store_experience(current_state, action, reward, next_state, game_over)
            
            # ğŸ“Š UPDATE TRACKING VARIABLES
            # ============================
            current_state = next_state
            total_reward = total_reward + reward
            steps_taken = steps_taken + 1
            
            # ğŸ¨ SHOW GAME IF ENABLED
            # =======================
            if show_training:
                env.render()
                time.sleep(0.01)  # Slow down so humans can watch
            
            # ğŸ“ LEARN FROM EXPERIENCES (if enough stored)
            # ===========================================
            if len(agent.memory) > agent.batch_size:
                agent.train_from_experience()
            
            # ğŸ CHECK IF EPISODE FINISHED
            # ============================
            if game_over:
                break
        
        # ğŸ“ˆ RECORD EPISODE RESULTS
        # ========================
        episode_score = int(info['score'])
        all_scores.append(episode_score)
        all_episode_lengths.append(steps_taken)
        agent.episode_scores.append(int(episode_score))
        
        # ğŸ“‰ DECAY EXPLORATION (once per episode, not per step!)
        # =====================================================
        agent.decay_epsilon()
        
        # ğŸ¯ UPDATE TARGET NETWORK PERIODICALLY
        # ====================================
        if episode_idx % target_update_frequency == 0:
            agent.copy_to_target_network()
        
        # ğŸ“Š DISPLAY PROGRESS UPDATES
        # ===========================
        show_progress = (episode_idx % 100 == 0 or episode_score > best_score_ever)
        
        if show_progress:
            episode_time = time.time() - episode_start_time
            
            # Calculate averages
            if len(all_scores) >= 100:
                avg_score = sum(all_scores[-100:]) / 100
                avg_length = sum(all_episode_lengths[-100:]) / 100
            else:
                avg_score = sum(all_scores) / len(all_scores)
                avg_length = sum(all_episode_lengths) / len(all_episode_lengths)
            
            print(f"Episode {episode_number:4d} | "
                  f"Score: {episode_score:3d} | "
                  f"Steps: {steps_taken:4d} | "
                  f"Avg Score: {avg_score:.2f} | "
                  f"Avg Steps: {avg_length:.1f} | "
                  f"Exploration: {agent.epsilon:.3f} | "
                  f"Time: {episode_time:.2f}s")
            
            # ğŸ† CHECK FOR NEW RECORD
            # =======================
            if episode_score > best_score_ever:
                best_score_ever = episode_score
                print(f"   ğŸ‰ NEW RECORD! Best score: {best_score_ever}")
        
        # ğŸ’¾ SAVE PROGRESS PERIODICALLY
        # =============================
        if episode_idx % save_frequency == 0 and episode_idx > 0:
            os.makedirs('models/checkpoints', exist_ok=True)
            checkpoint_filename = os.path.join('models', 'checkpoints', f'ai_driver_checkpoint_episode_{episode_number}.pth')
            agent.save_agent(checkpoint_filename)
            print(f"   ğŸ’¾ Progress saved: {checkpoint_filename}")
        
        # ğŸ“Š UPDATE CHARTS PERIODICALLY
        # =============================
        if episode_idx % chart_update_frequency == 0 and episode_idx > 0:
            print(f"   ğŸ“Š Updating training charts...")
            os.makedirs('results/charts', exist_ok=True)
            chart_path = os.path.join('results', 'charts', 'ai_training_progress.png')
            agent.create_training_charts(save_path=chart_path)
    
    # ğŸ† TRAINING COMPLETED OR INTERRUPTED!
    # =====================================
    total_training_time = time.time() - training_start_time
    actual_episodes = len(agent.episode_scores) - (len(agent.episode_scores) if not resume_checkpoint else len(agent.episode_scores) - start_episode)
    
    if graceful_shutdown:
        print("\n" + "=" * 50)
        print("ğŸ›‘ TRAINING GRACEFULLY INTERRUPTED")
        print("=" * 50)
        print("âœ… Current progress has been preserved!")
    else:
        print("\n" + "=" * 50)
        print("ğŸ“ TRAINING COMPLETED SUCCESSFULLY!")
        print("=" * 50)
    
    # ğŸ“Š FINAL STATISTICS
    # ==================
    if len(all_scores) > 0:
        final_avg_score = sum(all_scores[-100:]) / min(100, len(all_scores))
        print(f"ğŸ† Best score achieved: {best_score_ever}")
        print(f"ğŸ“ˆ Final average score: {final_avg_score:.2f}")
    else:
        print(f"ğŸ† Best score achieved: {best_score_ever}")
        print("ğŸ“ˆ No episodes completed")
        
    print(f"â±ï¸  Total training time: {total_training_time/60:.1f} minutes")
    print(f"ğŸ® Episodes completed: {len(agent.episode_scores)}")
    print(f"ğŸ§  Final exploration rate: {agent.epsilon:.4f}")
    
    # ğŸ’¾ SAVE CURRENT STATE (ALWAYS SAVE ON SHUTDOWN)
    # ===============================================
    os.makedirs('models/final', exist_ok=True)
    if graceful_shutdown:
        # Save with a special graceful shutdown name
        current_episode = len(agent.episode_scores)
        final_model_name = os.path.join('models', 'final', f'f1_race_ai_interrupted_episode_{current_episode}.pth')
        print(f"ğŸ’¾ Saving interrupted training state...")
    else:
        final_model_name = os.path.join('models', 'final', 'f1_race_ai_final_model.pth')
        print(f"ğŸ’¾ Saving final trained model...")
    
    agent.save_agent(final_model_name)
    
    # ğŸ“Š CREATE TRAINING CHARTS
    # =========================
    print("ğŸ“Š Creating training progress charts...")
    chart_path = os.path.join('results', 'charts', 'ai_training_progress.png')
    agent.create_training_charts(save_path=chart_path)
    print(f"ğŸ“Š Chart saved at: {chart_path}")
    
    # Don't show chart automatically - it blocks the program
    # User can view it separately or via the dashboard
    
    # ğŸšª CLEANUP
    # ==========
    env.close()
    
    if graceful_shutdown:
        print(f"ğŸ‰ Your AI training was safely interrupted and saved as '{final_model_name}'!")
        print("ğŸ’¡ You can resume training from this point using the 'resume' option.")
    else:
        print(f"ğŸ‰ Your AI race car driver is now trained and saved as '{final_model_name}'!")
    
    return agent

def test_trained_ai(model_path, num_test_episodes=5, show_games=True, framerate_multiplier: int = 100):
    """
    ğŸ§ª Test how well our trained AI performs!
    
    This function loads a trained AI and lets it play several games
    to see how skilled it has become.
    
    Args:
        model_path (str): Path to the trained AI model file
        num_test_episodes (int): How many test games to play
        show_games (bool): Whether to show the games visually
        framerate_multiplier (int): Speed multiplier 1-500 (100 = normal speed, 200 = 2x speed)
        
    Returns:
        list: Scores achieved in test episodes
    """
    print(f"ğŸ§ª TESTING TRAINED AI")
    print("=" * 30)
    print(f"ğŸ“‚ Loading model: {model_path}")
    print(f"ğŸ® Test episodes: {num_test_episodes}")
    
    # ğŸ—ï¸ CREATE TESTING ENVIRONMENT AND LOAD TRAINED AI
    # =================================================
    env = F1RaceEnvironment(render=show_games, framerate_multiplier=framerate_multiplier)
    agent = DQNAgent(
        state_size=env.state_space_size,
        action_size=env.action_space_size
    )
    
    # ğŸ“‚ LOAD THE TRAINED AI "BRAIN"
    # ==============================
    agent.load_agent(model_path)
    
    # ğŸ”§ PERFORMANCE FIX: Explicitly disable exploration
    original_epsilon = agent.epsilon
    agent.epsilon = 0.0  # Completely disable exploration for testing
    
    print("ğŸ¤– AI loaded successfully!")
    print(f"ğŸ”§ Original epsilon: {original_epsilon:.4f} â†’ Testing epsilon: {agent.epsilon:.4f}")
    print("ğŸ¯ Testing mode: No exploration (pure skill)")
    print("-" * 30)
    
    # ğŸ“Š TEST RESULTS TRACKING
    # =======================
    test_scores = []
    test_episode_lengths = []
    
    # ğŸ® RUN TEST EPISODES
    # ===================
    for episode in range(num_test_episodes):
        current_state = env.reset()
        total_reward = 0
        steps = 0
        
        print(f"\nğŸ® Test Episode {episode + 1}")
        print("   Watch the AI show off its skills! ğŸ†")
        
        # ğŸ PLAY ONE COMPLETE GAME
        # =========================
        action_counts = [0, 0, 0]  # Track action usage [stay, left, right]
        
        while True:
            # ğŸ¯ AI CHOOSES BEST ACTION (no randomness)
            # ========================================
            action = agent.choose_action(current_state, training_mode=False)
            action_counts[action] += 1  # Count this action
            
            # ğŸ¬ TAKE ACTION
            # ==============
            next_state, reward, game_over, info = env.step(action)
            
            # ğŸ“Š UPDATE TRACKING
            # ==================
            current_state = next_state
            total_reward = total_reward + reward
            steps = steps + 1
            
            # ğŸ¨ SHOW GAME
            # ============
            if show_games:
                env.render()
                time.sleep(0.03)  # Slower for better viewing
            
            # ğŸ CHECK IF GAME ENDED
            # ======================
            if game_over:
                break
        
        # ğŸ“Š RECORD TEST RESULTS
        # =====================
        final_score = info['score']
        test_scores.append(final_score)
        test_episode_lengths.append(steps)
        
        print(f"   âœ… Episode {episode + 1} Results:")
        print(f"      ğŸ† Score: {final_score}")
        print(f"      ğŸ“ Steps survived: {steps}")
        print(f"      ğŸ Total reward: {total_reward:.2f}")
        print(f"      ğŸ® Actions - Stay: {action_counts[0]}, Left: {action_counts[1]}, Right: {action_counts[2]}")
    
    # ğŸ“Š CALCULATE FINAL TEST STATISTICS
    # ==================================
    avg_score = sum(test_scores) / len(test_scores)
    best_test_score = max(test_scores)
    avg_length = sum(test_episode_lengths) / len(test_episode_lengths)
    
    print(f"\nğŸ† FINAL TEST RESULTS")
    print("=" * 25)
    print(f"ğŸ“Š Average Score: {avg_score:.2f}")
    print(f"ğŸ¥‡ Best Score: {best_test_score}")
    print(f"ğŸ“ Average Episode Length: {avg_length:.1f} steps")
    print(f"ğŸ¯ All Scores: {test_scores}")
    
    # ğŸšª CLEANUP
    # ==========
    env.close()
    return test_scores

def test_random_driver(num_episodes=10, framerate_multiplier: int = 100):
    """
    ğŸ² Test a completely random driver as baseline comparison
    
    This shows how badly a random (untrained) driver performs,
    which helps us appreciate how much our AI has learned!
    
    Args:
        num_episodes (int): Number of random games to play
        framerate_multiplier (int): Speed multiplier 1-500 (100 = normal speed, 200 = 2x speed)
        
    Returns:
        list: Scores achieved by random driver
    """
    print("ğŸ² TESTING RANDOM DRIVER (BASELINE)")
    print("=" * 40)
    print("This shows what happens without AI training! ğŸ˜…")
    
    # ğŸ® CREATE ENVIRONMENT
    # ====================
    env = F1RaceEnvironment(render=True, framerate_multiplier=framerate_multiplier)
    random_scores = []
    random_lengths = []
    
    # ğŸ® PLAY RANDOM GAMES
    # ===================
    for episode in range(num_episodes):
        env.reset()
        steps = 0
        
        print(f"\nğŸ² Random Episode {episode + 1}")
        print("   Watching chaos unfold... ğŸ’¥")
        
        while True:
            # ğŸ² CHOOSE COMPLETELY RANDOM ACTION
            # =================================
            random_action = np.random.choice(env.action_space_size)
            
            # ğŸ¬ TAKE RANDOM ACTION
            # ====================
            _, _, game_over, info = env.step(random_action)
            steps = steps + 1
            
            # ğŸ¨ SHOW THE CHAOS
            # =================
            env.render()
            time.sleep(0.03)
            
            if game_over:
                break
        
        # ğŸ“Š RECORD RANDOM RESULTS
        # =======================
        score = info['score']
        random_scores.append(score)
        random_lengths.append(steps)
        
        print(f"   ğŸ’¥ Episode {episode + 1}: Score = {score}, Steps = {steps}")
    
    # ğŸ“Š RANDOM DRIVER STATISTICS
    # ===========================
    avg_random_score = sum(random_scores) / len(random_scores)
    avg_random_length = sum(random_lengths) / len(random_lengths)
    
    print(f"\nğŸ“Š RANDOM DRIVER RESULTS:")
    print(f"   ğŸ“Š Average Score: {avg_random_score:.2f}")
    print(f"   ğŸ“ Average Length: {avg_random_length:.1f} steps")
    print(f"   ğŸ¯ All Scores: {random_scores}")
    print("   ğŸ’¡ This is why we need AI training! ğŸ¤–")
    
    env.close()
    return random_scores

# ğŸ® MAIN PROGRAM INTERFACE
# =========================
if __name__ == "__main__":
    print("ğŸï¸  F1 RACE AI TRAINING SYSTEM")
    print("=" * 50)
    print("Welcome to the AI Driver Training Center! ğŸ“")
    print()
    
    # ğŸ“‚ CHECK FOR EXISTING TRAINED MODELS
    # ====================================
    # Gather models from root and models directories
    saved_models = []
    root_models = [f for f in os.listdir('.') if f.endswith('.pth')]
    saved_models.extend(root_models)
    for subdir in ['models', os.path.join('models', 'final'), os.path.join('models', 'checkpoints')]:
        if os.path.isdir(subdir):
            for f in os.listdir(subdir):
                if f.endswith('.pth'):
                    saved_models.append(os.path.join(subdir, f))
    
    if saved_models:
        print("ğŸ“‚ Found existing trained models:")
        for i, model in enumerate(saved_models):
            print(f"   {i}: {model}")
        print()
    
    # ğŸ¯ MENU OPTIONS
    # ===============
    print("ğŸ¯ What would you like to do?")
    print("   ğŸ“š 'train'    - Train a new AI driver from scratch (with framerate & chart options)")
    print("   ğŸ§ª 'test'     - Test an existing trained model (with framerate control)") 
    print("   ğŸ” 'resume'   - Resume training from a checkpoint (with framerate & chart options)")
    print("   ğŸ–¼ï¸  'chart'    - View the last training chart if available")
    print("   ï¿½ 'dashboard'- Launch web dashboard for real-time training monitoring")
    print("   ï¿½ğŸ² 'baseline' - Watch a random (untrained) driver fail (with framerate control)")
    print()
    print("   ğŸ’¡ NEW: Framerate multiplier (1-500%) controls training/testing speed!")
    print("   ğŸ’¡ NEW: Charts update periodically during training for real-time progress!")
    print("   ğŸ’¡ NEW: Adaptive exploration decay automatically scales to any training length!")
    print("   ğŸ’¡ NEW: Web dashboard for monitoring training without terminal clutter!")
    print()
    
    # ğŸ‘¤ GET USER CHOICE
    # ==================
    user_choice = input("Enter your choice: ").lower().strip()
    
    # ğŸš€ EXECUTE USER CHOICE
    # ======================
    if user_choice == 'train':
        print("\nğŸ‹ï¸ TRAINING MODE SELECTED")
        print("-" * 30)
        
        # Get training parameters
        show_visual = input("ğŸ¨ Show training visually? (y/n, default=n): ").lower().strip() == 'y'
        episodes_input = input("ğŸ® Number of episodes (default=2000): ").strip()
        episodes = int(episodes_input) if episodes_input else 2000
        
        # Get framerate configuration
        if show_visual:
            framerate_input = input("âš¡ Framerate multiplier 1-500% (default=100): ").strip()
            framerate = int(framerate_input) if framerate_input and framerate_input.isdigit() else 100
            framerate = max(1, min(500, framerate))  # Clamp to valid range
        else:
            framerate = 500  # Max speed for headless training
            print("âš¡ Using maximum framerate (500%) for headless training")
        
        # Get chart update frequency
        chart_freq_input = input("ğŸ“Š Chart update frequency in episodes (default=100): ").strip()
        chart_freq = int(chart_freq_input) if chart_freq_input and chart_freq_input.isdigit() else 100
        
        # Get exploration completion ratio
        exploration_input = input("ğŸ¯ Exploration completion % (at what % of training should exploration reach minimum, default=80): ").strip()
        exploration_ratio = float(exploration_input) / 100.0 if exploration_input and exploration_input.replace('.', '').isdigit() else 0.8
        exploration_ratio = max(0.1, min(1.0, exploration_ratio))  # Clamp between 10% and 100%
        
        # Get exploration decay type
        decay_type_input = input("ğŸ“‰ Exploration decay type (exponential/linear, default=exponential): ").strip().lower()
        decay_type = "linear" if decay_type_input == "linear" else "exponential"
        
        # Start training
        print(f"\nğŸš€ Starting training for {episodes} episodes...")
        print(f"   âš¡ Framerate: {framerate}%")
        print(f"   ğŸ“Š Charts will update every {chart_freq} episodes")
        print(f"   ğŸ¯ Exploration will reach minimum at {exploration_ratio*100:.0f}% of training")
        print(f"   ğŸ“‰ Using {decay_type} decay curve")
        trained_agent = train_racing_ai(episodes=episodes, show_training=show_visual, 
                                       framerate_multiplier=framerate, chart_update_frequency=chart_freq,
                                       exploration_completion_ratio=exploration_ratio, exploration_decay_type=decay_type)
        
        # Offer to test the newly trained agent
        test_new = input("\nğŸ§ª Test the newly trained AI? (y/n): ").lower().strip() == 'y'
        if test_new:
            final_model_name = os.path.join('models', 'final', 'f1_race_ai_final_model.pth')
            test_trained_ai(final_model_name, num_test_episodes=5, framerate_multiplier=100)
    
    elif user_choice == 'test' and saved_models:
        print("\nğŸ§ª TESTING MODE SELECTED")  
        print("-" * 25)
        
        # Show available models
        print("ğŸ“‚ Available models:")
        for i, model in enumerate(saved_models):
            print(f"   {i}: {model}")
        
        # Get model selection
        model_choice = int(input("ğŸ¯ Select model number: "))
        selected_model = saved_models[model_choice]
        
        # Get test parameters  
        test_episodes_input = input("ğŸ® Number of test episodes (default=5): ").strip()
        test_episodes = int(test_episodes_input) if test_episodes_input else 5
        
        # Get framerate configuration for testing
        framerate_input = input("âš¡ Framerate multiplier 1-500% (default=100): ").strip()
        framerate = int(framerate_input) if framerate_input and framerate_input.isdigit() else 100
        framerate = max(1, min(500, framerate))
        
        # Run test
        test_trained_ai(selected_model, num_test_episodes=test_episodes, framerate_multiplier=framerate)
    
    elif user_choice == 'resume':
        print("\nğŸ” RESUME MODE SELECTED")
        print("-" * 30)
        # Filter checkpoints - include regular checkpoints AND interrupted training files
        checkpoints = []
        for m in saved_models:
            filename = os.path.basename(m)
            # Include regular checkpoints, interrupted training files, and any file with "episode" in name
            if ('checkpoint' in filename or 
                'interrupted_episode' in filename or 
                'ai_driver_checkpoint_episode_' in filename):
                checkpoints.append(m)
        
        # Sort checkpoints by modification time (most recent first) 
        checkpoints.sort(key=lambda x: os.path.getmtime(x), reverse=True)
        
        if not checkpoints:
            print("âŒ No checkpoints found. Train first to create checkpoints.")
        else:
            print("ğŸ“‚ Available checkpoints (most recent first):")
            for i, model in enumerate(checkpoints):
                # Show modification time for clarity
                mod_time = os.path.getmtime(model)
                mod_time_str = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(mod_time))
                print(f"   {i}: {model} ({mod_time_str})")
            idx = int(input("ğŸ¯ Select checkpoint number: "))
            resume_path = checkpoints[idx]
            show_visual = input("ğŸ¨ Show training visually? (y/n, default=n): ").lower().strip() == 'y'
            episodes_input = input("ğŸ® Additional episodes to train (default=500): ").strip()
            episodes = int(episodes_input) if episodes_input else 500
            
            # Get framerate configuration  
            if show_visual:
                framerate_input = input("âš¡ Framerate multiplier 1-500% (default=100): ").strip()
                framerate = int(framerate_input) if framerate_input and framerate_input.isdigit() else 100
                framerate = max(1, min(500, framerate))
            else:
                framerate = 500  # Max speed for headless training
                print("âš¡ Using maximum framerate (500%) for headless training")
            
            # Get chart update frequency
            chart_freq_input = input("ğŸ“Š Chart update frequency in episodes (default=100): ").strip()
            chart_freq = int(chart_freq_input) if chart_freq_input and chart_freq_input.isdigit() else 100
            
            # Get exploration completion ratio
            exploration_input = input("ğŸ¯ Exploration completion % (at what % of training should exploration reach minimum, default=80): ").strip()
            exploration_ratio = float(exploration_input) / 100.0 if exploration_input and exploration_input.replace('.', '').isdigit() else 0.8
            exploration_ratio = max(0.1, min(1.0, exploration_ratio))  # Clamp between 10% and 100%
            
            # Get exploration decay type
            decay_type_input = input("ğŸ“‰ Exploration decay type (exponential/linear, default=exponential): ").strip().lower()
            decay_type = "linear" if decay_type_input == "linear" else "exponential"
            
            print(f"\nğŸš€ Resuming training for {episodes} episodes from {resume_path}...")
            print(f"   âš¡ Framerate: {framerate}%")  
            print(f"   ğŸ“Š Charts will update every {chart_freq} episodes")
            print(f"   ğŸ¯ Exploration will reach minimum at {exploration_ratio*100:.0f}% of training")
            print(f"   ğŸ“‰ Using {decay_type} decay curve")
            trained_agent = train_racing_ai(episodes=episodes, show_training=show_visual, resume_checkpoint=resume_path,
                           framerate_multiplier=framerate, chart_update_frequency=chart_freq,
                           exploration_completion_ratio=exploration_ratio, exploration_decay_type=decay_type)
            
            # Offer to test the newly trained agent
            test_new = input("\nğŸ§ª Test the newly trained AI? (y/n): ").lower().strip() == 'y'
            if test_new:
                final_model_name = os.path.join('models', 'final', 'f1_race_ai_final_model.pth')
                test_trained_ai(final_model_name, num_test_episodes=5, framerate_multiplier=100)

    elif user_choice == 'chart':
        print("\nğŸ–¼ï¸  VIEW CHART MODE")
        print("-" * 20)
        chart_path = os.path.join('results', 'charts', 'ai_training_progress.png')
        if os.path.isfile(chart_path):
            try:
                img = plt.imread(chart_path)
                plt.figure(figsize=(8, 6))
                plt.imshow(img)
                plt.axis('off')
                plt.title('Last Training Progress')
                plt.show()
            except Exception as _err:
                print(f"ğŸ“Š Chart saved at: {chart_path}")
        else:
            print("âŒ No chart found yet. Train first to generate one.")

    elif user_choice == 'dashboard':
        print("\nğŸŒ DASHBOARD MODE SELECTED")
        print("-" * 30)
        print("ğŸš€ Starting web dashboard for real-time training monitoring...")
        print()
        print("ğŸ“Š The dashboard provides:")
        print("   â€¢ Real-time training charts")
        print("   â€¢ Live statistics and metrics")
        print("   â€¢ Training status monitoring")
        print("   â€¢ Model information")
        print()
        print("ğŸŒ Dashboard will open at: http://localhost:5000")
        print("ğŸ’¡ Run training in another terminal: python train_ai.py")
        print()
        
        try:
            # Use the simple standalone dashboard
            import subprocess
            import sys
            dashboard_script = os.path.join(os.path.dirname(__file__), 'dashboard_simple.py')
            if os.path.exists(dashboard_script):
                print("ğŸ¯ Launching standalone dashboard...")
                subprocess.run([sys.executable, dashboard_script])
            else:
                print("âŒ Dashboard script not found!")
                print("ğŸ’¡ Try running: python dashboard_simple.py")
                
        except KeyboardInterrupt:
            print("\nğŸ›‘ Dashboard stopped")
        except Exception as e:
            print(f"âŒ Error starting dashboard: {e}")
            print("ğŸ’¡ Try running: python dashboard_simple.py")

    elif user_choice == 'baseline':
        print("\nğŸ² BASELINE MODE SELECTED")
        print("-" * 30)
        
        baseline_episodes_input = input("ğŸ® Number of random episodes (default=10): ").strip()
        baseline_episodes = int(baseline_episodes_input) if baseline_episodes_input else 10
        
        # Get framerate configuration for baseline
        framerate_input = input("âš¡ Framerate multiplier 1-500% (default=100): ").strip()
        framerate = int(framerate_input) if framerate_input and framerate_input.isdigit() else 100
        framerate = max(1, min(500, framerate))
        
        print("\nâš ï¸  Warning: This will be painful to watch! ğŸ˜…")
        input("Press Enter to continue...")
        
        test_random_driver(num_episodes=baseline_episodes, framerate_multiplier=framerate)
    
    elif user_choice == 'test' and not saved_models:
        print("\nâŒ No trained models found!")
        print("   ğŸ’¡ Please train a model first using 'train' option")
    
    else:
        print("\nâŒ Invalid choice or no models available")
        print("   ğŸ’¡ Valid options: 'train', 'test', 'baseline'")
    
    print("\nğŸ‰ Thanks for using the F1 Race AI Training System!")
    print("ğŸ‘‹ Happy AI training! ğŸ¤–ğŸï¸")